{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "46246386-fec1-4d42-bf10-698a4054dbda"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.util\n",
    "import dateutil.parser\n",
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import iris.pandas\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_commons import EXPERIMENTS, GASES\n",
    "from util_mypaths import path_to_atom, path_to_processed, path_to_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", module=\"iris\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"xarray\")\n",
    "plt.rcParams[\"mathtext.default\"] = \"regular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switches\n",
    "atom_yrmn = 1608  # 1608 or 1702\n",
    "ukca_mn = \"Aug\"  # Aug or Feb\n",
    "t0, t1 = 12, 120  # used in thesis: 24, 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select flights\n",
    "fname_atom = f\"MER-WAS_DC8_20{atom_yrmn}*.nc\"\n",
    "# Read ATom data\n",
    "kw_mf = dict(combine=\"nested\", concat_dim=\"time\", decode_cf=True)\n",
    "atom_dsinf = xr.open_mfdataset(sorted(path_to_atom.glob(fname_atom)), decode_times=True)\n",
    "atom_dsmms = xr.open_mfdataset(sorted(path_to_atom.glob(fname_atom)), group=\"MMS\", **kw_mf)\n",
    "atom_dswas = xr.open_mfdataset(sorted(path_to_atom.glob(fname_atom)), group=\"WAS\", **kw_mf)\n",
    "atom_dsch4 = xr.open_mfdataset(sorted(path_to_atom.glob(fname_atom)), group=\"NOAA-Picarro\", **kw_mf)\n",
    "atom_dsno = xr.open_mfdataset(sorted(path_to_atom.glob(fname_atom)), group=\"NOyO3-NO\", **kw_mf)\n",
    "atom_dsno2 = xr.open_mfdataset(sorted(path_to_atom.glob(fname_atom)), group=\"NOyO3-NO2\", **kw_mf)\n",
    "atom_dsnoy = xr.open_mfdataset(sorted(path_to_atom.glob(fname_atom)), group=\"NOyO3-NOy\", **kw_mf)\n",
    "atom_dso3 = xr.open_mfdataset(sorted(path_to_atom.glob(fname_atom)), group=\"NOyO3-O3\", **kw_mf)\n",
    "atom_dsoh = xr.open_mfdataset(sorted(path_to_atom.glob(fname_atom)), group=\"ATHOS-HOx\", **kw_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose experiment\n",
    "exp = \"CHEM\"  # BASE; DONE CHEM, MARI, FIRE, FULL\n",
    "path_to_exp = path_to_processed / EXPERIMENTS[exp] / \"releveled\"\n",
    "fname_ukca = f\"{EXPERIMENTS[exp]}_relvl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "5bf220d9-75c8-489e-81fb-58c0a9a4f83d"
    }
   },
   "outputs": [],
   "source": [
    "# Read UKCA data\n",
    "cb_nyrs_ch4 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_ch4.nc\")), \"ch4\")\n",
    "cb_nyrs_c2h6 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_c2h6.nc\")), \"c2h6\")\n",
    "cb_nyrs_c3h8 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_c3h8.nc\")), \"c3h8\")\n",
    "\n",
    "cb_nyrs_no = iris.load_cube(str(path_to_exp / (fname_ukca + \"_no.nc\")), \"no\")\n",
    "cb_nyrs_no2 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_no2.nc\")), \"no2\")\n",
    "cb_nyrs_hono = iris.load_cube(str(path_to_exp / (fname_ukca + \"_hono.nc\")), \"hono\")\n",
    "cb_nyrs_ho2no2 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_ho2no2.nc\")), \"ho2no2\")\n",
    "cb_nyrs_hno3 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_hno3.nc\")), \"hno3\")\n",
    "cb_nyrs_n2o5 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_n2o5.nc\")), \"n2o5\")\n",
    "cb_nyrs_pan = iris.load_cube(str(path_to_exp / (fname_ukca + \"_pan.nc\")), \"pan\")\n",
    "cb_nyrs_ppan = iris.load_cube(str(path_to_exp / (fname_ukca + \"_ppan.nc\")), \"ppan\")\n",
    "\n",
    "cb_nyrs_meono2 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_meono2.nc\")), \"meono2\")\n",
    "cb_nyrs_etono2 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_etono2.nc\")), \"etono2\")\n",
    "cb_nyrs_nprono2 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_nprono2.nc\")), \"nprono2\")\n",
    "cb_nyrs_iprono2 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_iprono2.nc\")), \"iprono2\")\n",
    "\n",
    "cb_nyrs_o3 = iris.load_cube(str(path_to_exp / (fname_ukca + \"_o3.nc\")), \"o3\")\n",
    "cb_nyrs_oh = iris.load_cube(str(path_to_exp / (fname_ukca + \"_oh.nc\")), \"oh\")\n",
    "\n",
    "# Read UKCA horizontal coordinates\n",
    "ukca_lats = iris.load_cube(\n",
    "    str(path_to_raw / \"um_orography_xnvtj.nc\"), \"OROGRAPHY (/STRAT LOWER BC)\"\n",
    ").coord(\"latitude\")\n",
    "ukca_lons = iris.load_cube(\n",
    "    str(path_to_raw / \"um_orography_xnvtj.nc\"), \"OROGRAPHY (/STRAT LOWER BC)\"\n",
    ").coord(\"longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select geographical regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetimes from ATom data\n",
    "atom_datetime = atom_dsinf.time.values.astype(\"<M8[us]\").astype(datetime)\n",
    "atom_date_strt = atom_datetime[0]\n",
    "atom_date_stop = atom_datetime[-1]\n",
    "# Extract spatial coordinates from ATom data\n",
    "sample_lats = np.asarray(atom_dsmms.G_LAT.data)\n",
    "sample_lons = np.asarray(atom_dsmms.G_LONG.data)\n",
    "sample_alts = np.asarray(atom_dsmms.G_ALT.data)\n",
    "# Construct pairs of coordinate points\n",
    "sample_lon_lat_points = []\n",
    "for i, j in zip(sample_lons, sample_lats):\n",
    "    sample_lon_lat_points.append(Point(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a geographical region\n",
    "r1_llon_ukca, r1_ulon_ukca = 195.9375, 214.6875\n",
    "r1_llon, r1_ulon = r1_llon_ukca - 360, r1_ulon_ukca - 360\n",
    "r1_llat, r1_ulat = 21.875, 53.125\n",
    "r1 = Polygon([(r1_llon, r1_llat), (r1_llon, r1_ulat), (r1_ulon, r1_ulat), (r1_ulon, r1_llat)])\n",
    "r2_llon_ukca, r2_ulon_ukca = 169.6875, 201.5625\n",
    "r2_1_llon, r2_1_ulon = r2_llon_ukca, 180\n",
    "r2_2_llon, r2_2_ulon = -180, r2_ulon_ukca - 360\n",
    "r2_llat, r2_ulat = -40.625, 20.625\n",
    "r2_1 = Polygon(\n",
    "    [(r2_1_llon, r2_llat), (r2_1_llon, r2_ulat), (r2_1_ulon, r2_ulat), (r2_1_ulon, r2_llat)]\n",
    ")\n",
    "r2_2 = Polygon(\n",
    "    [(r2_2_llon, r2_llat), (r2_2_llon, r2_ulat), (r2_2_ulon, r2_ulat), (r2_2_ulon, r2_llat)]\n",
    ")\n",
    "r3_llon_ukca, r3_ulon_ukca = 180.9375, 285.9375\n",
    "r3_llon, r3_ulon = r3_llon_ukca - 360, r3_ulon_ukca - 360\n",
    "r3_llat, r3_ulat = -70.625, -50.625\n",
    "r3 = Polygon([(r3_llon, r3_llat), (r3_llon, r3_ulat), (r3_ulon, r3_ulat), (r3_ulon, r3_llat)])\n",
    "r4_llon_ukca, r4_ulon_ukca = 312.1875, 334.6875\n",
    "r4_llon, r4_ulon = r4_llon_ukca - 360, r4_ulon_ukca - 360\n",
    "r4_llat, r4_ulat = -45.625, -25.625\n",
    "r4 = Polygon([(r4_llon, r4_llat), (r4_llon, r4_ulat), (r4_ulon, r4_ulat), (r4_ulon, r4_llat)])\n",
    "r5_llon_ukca, r5_ulon_ukca = 323.4375, 344.0625\n",
    "r5_llon, r5_ulon = r5_llon_ukca - 360, r5_ulon_ukca - 360\n",
    "r5_llat, r5_ulat = -6.875, 36.875\n",
    "r5 = Polygon([(r5_llon, r5_llat), (r5_llon, r5_ulat), (r5_ulon, r5_ulat), (r5_ulon, r5_llat)])\n",
    "r6_llon_ukca, r6_ulon_ukca = 321.5625, 338.4375\n",
    "r6_llon, r6_ulon = r6_llon_ukca - 360, r6_ulon_ukca - 360\n",
    "r6_llat, r6_ulat = 38.125, 63.125\n",
    "r6 = Polygon([(r6_llon, r6_llat), (r6_llon, r6_ulat), (r6_ulon, r6_ulat), (r6_ulon, r6_llat)])\n",
    "r7_llon_ukca, r7_ulon_ukca = 269.0625, 321.5625\n",
    "r7_llon, r7_ulon = r7_llon_ukca - 360, r7_ulon_ukca - 360\n",
    "r7_llat, r7_ulat = 59.375, 81.875\n",
    "r7 = Polygon([(r7_llon, r7_llat), (r7_llon, r7_ulat), (r7_ulon, r7_ulat), (r7_ulon, r7_llat)])\n",
    "# r8_llon_ukca, r8_ulon_ukca = 237.1875, 269.0625\n",
    "# r8_llon, r8_ulon = r8_llon_ukca-360, r8_ulon_ukca-360\n",
    "# r8_llat, r8_ulat = 31.875, 50.625\n",
    "# r8 = Polygon([(r8_llon, r8_llat), (r8_llon, r8_ulat), (r8_ulon, r8_ulat), (r8_ulon, r8_llat)])\n",
    "r9_llon_ukca, r9_ulon_ukca = (\n",
    "    201.5625,\n",
    "    233.4375,\n",
    ")  # between 234 and 237.1875 there is a spike in alkanes!\n",
    "r9_llon, r9_ulon = r9_llon_ukca - 360, r9_ulon_ukca - 360\n",
    "r9_llat, r9_ulat = 61.875, 81.875\n",
    "r9 = Polygon([(r9_llon, r9_llat), (r9_llon, r9_ulat), (r9_ulon, r9_ulat), (r9_ulon, r9_llat)])\n",
    "# Find points within a region\n",
    "r1_points_within = []\n",
    "r2_1_points_within, r2_2_points_within = [], []\n",
    "r3_points_within = []\n",
    "r4_points_within = []\n",
    "r5_points_within = []\n",
    "r6_points_within = []\n",
    "r7_points_within = []\n",
    "# r8_points_within = []\n",
    "r9_points_within = []\n",
    "for p in sample_lon_lat_points:\n",
    "    r1_points_within.append(p.within(r1))\n",
    "    r2_1_points_within.append(p.within(r2_1))\n",
    "    r2_2_points_within.append(p.within(r2_2))\n",
    "    r3_points_within.append(p.within(r3))\n",
    "    r4_points_within.append(p.within(r4))\n",
    "    r5_points_within.append(p.within(r5))\n",
    "    r6_points_within.append(p.within(r6))\n",
    "    r7_points_within.append(p.within(r7))\n",
    "    #     r8_points_within.append(p.within(r8))\n",
    "    r9_points_within.append(p.within(r9))\n",
    "r2_points_within = list(np.asarray(r2_1_points_within) | np.asarray(r2_2_points_within))\n",
    "# Combine points within and region corners into lists\n",
    "points_within_regions = [\n",
    "    r1_points_within,\n",
    "    r2_points_within,\n",
    "    r3_points_within,\n",
    "    r4_points_within,\n",
    "    r5_points_within,\n",
    "    r6_points_within,\n",
    "    r7_points_within,\n",
    "    r9_points_within,\n",
    "]\n",
    "ukca_regions_corners = [\n",
    "    [r1_llon_ukca, r1_ulon_ukca, r1_llat, r1_ulat],\n",
    "    [r2_llon_ukca, r2_ulon_ukca, r2_llat, r2_ulat],\n",
    "    [r3_llon_ukca, r3_ulon_ukca, r3_llat, r3_ulat],\n",
    "    [r4_llon_ukca, r4_ulon_ukca, r4_llat, r4_ulat],\n",
    "    [r5_llon_ukca, r5_ulon_ukca, r5_llat, r5_ulat],\n",
    "    [r6_llon_ukca, r6_ulon_ukca, r6_llat, r6_ulat],\n",
    "    [r7_llon_ukca, r7_ulon_ukca, r7_llat, r7_ulat],\n",
    "    [r9_llon_ukca, r9_ulon_ukca, r9_llat, r9_ulat],\n",
    "]\n",
    "region_names = [\n",
    "    \"N Pacific\",\n",
    "    \"Central Pacific\",\n",
    "    \"S Pacific\",\n",
    "    \"SE Atlantic\",\n",
    "    \"Central Atlantic\",\n",
    "    \"N Atlantic\",\n",
    "    \"Greenland\",\n",
    "    \"W Canada\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process ATom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ATom C2H6, C2H4, C2H2, C3H8, C3H6 to pptvC to match UKCA lumped species, namely C2H6=C2H6+C2H4+C2H2, C3H8=C3H8+C3H6\n",
    "atom_dswas_c2h6 = (\n",
    "    atom_dswas[\"Ethane_WAS\"] * 2 + atom_dswas[\"Ethene_WAS\"] * 2 + atom_dswas[\"Ethyne_WAS\"] * 2\n",
    ")\n",
    "atom_dswas_c3h8 = atom_dswas[\"Propane_WAS\"]  # *3 + atom_dswas['Propene_WAS']*3\n",
    "# Convert xarray dataset with observational data to pandas dataframe\n",
    "atom_ch4 = atom_dsch4.CH4_NOAA.to_dataframe(name=\"ch4\")\n",
    "atom_c2h6 = atom_dswas_c2h6.to_dataframe(name=\"c2h6\")\n",
    "atom_c3h8 = atom_dswas_c3h8.to_dataframe(name=\"c3h8\")\n",
    "atom_o3 = atom_dso3[\"O3_CL\"].to_dataframe(name=\"o3\")\n",
    "atom_oh = atom_dsoh[\"OH_pptv\"].to_dataframe(name=\"oh\")\n",
    "atom_no = atom_dsno[\"NO_CL\"].to_dataframe(name=\"no\")\n",
    "atom_dsnox = atom_dsno[\"NO_CL\"] + atom_dsno2[\"NO2_CL\"]\n",
    "atom_nox = atom_dsnox.to_dataframe(name=\"nox\")\n",
    "atom_noy = atom_dsnoy[\"NOy_CL\"].to_dataframe(name=\"noy\")\n",
    "atom_meono2 = atom_dswas[\"MeONO2_WAS\"].to_dataframe(name=\"meono2\")\n",
    "atom_etono2 = atom_dswas[\"EtONO2_WAS\"].to_dataframe(name=\"etono2\")\n",
    "atom_nprono2 = atom_dswas[\"n-PrONO2_WAS\"].to_dataframe(name=\"nprono2\")\n",
    "atom_iprono2 = atom_dswas[\"i-PrONO2_WAS\"].to_dataframe(name=\"iprono2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select longitudes and latitudes where ATom data is available\n",
    "lons_ch4_notnan = np.where(atom_ch4.ch4.isna() == False, sample_lons, atom_ch4.ch4)\n",
    "lats_ch4_notnan = np.where(atom_ch4.ch4.isna() == False, sample_lats, atom_ch4.ch4)\n",
    "lons_c2h6_notnan = np.where(atom_c2h6.c2h6.isna() == False, sample_lons, atom_c2h6.c2h6)\n",
    "lats_c2h6_notnan = np.where(atom_c2h6.c2h6.isna() == False, sample_lats, atom_c2h6.c2h6)\n",
    "lons_c3h8_notnan = np.where(atom_c3h8.c3h8.isna() == False, sample_lons, atom_c3h8.c3h8)\n",
    "lats_c3h8_notnan = np.where(atom_c3h8.c3h8.isna() == False, sample_lats, atom_c3h8.c3h8)\n",
    "lons_o3_notnan = np.where(atom_o3.o3.isna() == False, sample_lons, atom_o3.o3)\n",
    "lats_o3_notnan = np.where(atom_o3.o3.isna() == False, sample_lats, atom_o3.o3)\n",
    "lons_oh_notnan = np.where(atom_oh.oh.isna() == False, sample_lons, atom_oh.oh)\n",
    "lats_oh_notnan = np.where(atom_oh.oh.isna() == False, sample_lats, atom_oh.oh)\n",
    "lons_no_notnan = np.where(atom_no.no.isna() == False, sample_lons, atom_no.no)\n",
    "lats_no_notnan = np.where(atom_no.no.isna() == False, sample_lats, atom_no.no)\n",
    "lons_nox_noxtnan = np.where(atom_nox.nox.isna() == False, sample_lons, atom_nox.nox)\n",
    "lats_nox_noxtnan = np.where(atom_nox.nox.isna() == False, sample_lats, atom_nox.nox)\n",
    "lons_noy_noytnan = np.where(atom_noy.noy.isna() == False, sample_lons, atom_noy.noy)\n",
    "lats_noy_noytnan = np.where(atom_noy.noy.isna() == False, sample_lats, atom_noy.noy)\n",
    "lons_meono2_notnan = np.where(atom_meono2.meono2.isna() == False, sample_lons, atom_meono2.meono2)\n",
    "lats_meono2_notnan = np.where(atom_meono2.meono2.isna() == False, sample_lats, atom_meono2.meono2)\n",
    "lons_etono2_notnan = np.where(atom_etono2.etono2.isna() == False, sample_lons, atom_etono2.etono2)\n",
    "lats_etono2_notnan = np.where(atom_etono2.etono2.isna() == False, sample_lats, atom_etono2.etono2)\n",
    "lons_nprono2_notnan = np.where(\n",
    "    atom_nprono2.nprono2.isna() == False, sample_lons, atom_nprono2.nprono2\n",
    ")\n",
    "lats_nprono2_notnan = np.where(\n",
    "    atom_nprono2.nprono2.isna() == False, sample_lats, atom_nprono2.nprono2\n",
    ")\n",
    "lons_iprono2_notnan = np.where(\n",
    "    atom_iprono2.iprono2.isna() == False, sample_lons, atom_iprono2.iprono2\n",
    ")\n",
    "lats_iprono2_notnan = np.where(\n",
    "    atom_iprono2.iprono2.isna() == False, sample_lats, atom_iprono2.iprono2\n",
    ")\n",
    "lons_notnan = [\n",
    "    lons_ch4_notnan,\n",
    "    lons_c2h6_notnan,\n",
    "    lons_c3h8_notnan,\n",
    "    lons_o3_notnan,\n",
    "    lons_oh_notnan,\n",
    "    lons_no_notnan,\n",
    "    lons_nox_noxtnan,\n",
    "    lons_meono2_notnan,\n",
    "    lons_etono2_notnan,\n",
    "    lons_nprono2_notnan,\n",
    "    lons_iprono2_notnan,\n",
    "]\n",
    "lats_notnan = [\n",
    "    lats_ch4_notnan,\n",
    "    lats_c2h6_notnan,\n",
    "    lats_c3h8_notnan,\n",
    "    lats_o3_notnan,\n",
    "    lats_oh_notnan,\n",
    "    lats_no_notnan,\n",
    "    lats_nox_noxtnan,\n",
    "    lats_meono2_notnan,\n",
    "    lats_etono2_notnan,\n",
    "    lats_nprono2_notnan,\n",
    "    lats_iprono2_notnan,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin observational data\n",
    "alt_bins = np.arange(0, 14000, 500)\n",
    "alt_bin_inds = np.arange(0, len(alt_bins) - 1, 1)\n",
    "alt_bin_mids = np.arange(250, 13500, 500)\n",
    "alt_r1_bin_inds = np.digitize(atom_dsmms.G_ALT.data[r1_points_within], alt_bins)\n",
    "alt_r2_bin_inds = np.digitize(atom_dsmms.G_ALT.data[r2_points_within], alt_bins)\n",
    "alt_r3_bin_inds = np.digitize(atom_dsmms.G_ALT.data[r3_points_within], alt_bins)\n",
    "alt_r4_bin_inds = np.digitize(atom_dsmms.G_ALT.data[r4_points_within], alt_bins)\n",
    "alt_r5_bin_inds = np.digitize(atom_dsmms.G_ALT.data[r5_points_within], alt_bins)\n",
    "alt_r6_bin_inds = np.digitize(atom_dsmms.G_ALT.data[r6_points_within], alt_bins)\n",
    "alt_r7_bin_inds = np.digitize(atom_dsmms.G_ALT.data[r7_points_within], alt_bins)\n",
    "# alt_r8_bin_inds = np.digitize(atom_dsmms.G_ALT.data[r8_points_within], alt_bins)\n",
    "alt_r9_bin_inds = np.digitize(atom_dsmms.G_ALT.data[r9_points_within], alt_bins)\n",
    "alt_regions_bin_inds = [\n",
    "    alt_r1_bin_inds,\n",
    "    alt_r2_bin_inds,\n",
    "    alt_r3_bin_inds,\n",
    "    alt_r4_bin_inds,\n",
    "    alt_r5_bin_inds,\n",
    "    alt_r6_bin_inds,\n",
    "    alt_r7_bin_inds,\n",
    "    alt_r9_bin_inds,\n",
    "]\n",
    "# alt_regions_bin_inds = [alt_r1_bin_inds, alt_r2_bin_inds, alt_r3_bin_inds,\n",
    "#                         alt_r4_bin_inds, alt_r5_bin_inds, alt_r6_bin_inds,\n",
    "#                         alt_r7_bin_inds, alt_r8_bin_inds, alt_r9_bin_inds]\n",
    "# Calculate mean and standard deviation\n",
    "atom2process = [\n",
    "    atom_ch4,\n",
    "    atom_c2h6,\n",
    "    atom_c3h8,\n",
    "    atom_o3,\n",
    "    atom_oh,\n",
    "    atom_no,\n",
    "    atom_nox,\n",
    "    atom_noy,\n",
    "    atom_meono2,\n",
    "    atom_etono2,\n",
    "    atom_nprono2,\n",
    "    atom_iprono2,\n",
    "]\n",
    "atom_regional_vps = []\n",
    "for points_within_region, alt_region_bin_inds in zip(points_within_regions, alt_regions_bin_inds):\n",
    "    specie_dict = {}\n",
    "    for specie in atom2process:\n",
    "        stat_dict = {}\n",
    "        stat_dict[\"mean\"] = (\n",
    "            specie[points_within_region]\n",
    "            .groupby(alt_region_bin_inds.compute())\n",
    "            .mean()\n",
    "            .reindex(index=alt_bin_inds, fill_value=np.nan)\n",
    "        )\n",
    "        stat_dict[\"std\"] = (\n",
    "            specie[points_within_region]\n",
    "            .groupby(alt_region_bin_inds.compute())\n",
    "            .std()\n",
    "            .reindex(index=alt_bin_inds, fill_value=np.nan)\n",
    "        )\n",
    "        stat_dict[\"count\"] = (\n",
    "            specie[points_within_region]\n",
    "            .groupby(alt_region_bin_inds.compute())\n",
    "            .count()\n",
    "            .reindex(index=alt_bin_inds, fill_value=np.nan)\n",
    "        )\n",
    "        specie_dict[specie.columns[0]] = stat_dict\n",
    "    atom_regional_vps.append(specie_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store ATom vertical profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_prcd = Path.home() / \"uea\" / \"phd\" / \"models\" / \"ukca\" / \"processed\" / \"vertical_profiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memo: atom_regional_vps[r]['sp_key']['mean','std_dev']\n",
    "for ir, vp in enumerate(atom_regional_vps):\n",
    "    for vp_key, vp_value in vp.items():\n",
    "        cb_mean = iris.cube.Cube(\n",
    "            vp_value[\"mean\"].values.squeeze(),\n",
    "            long_name=vp_key,\n",
    "            dim_coords_and_dims=[\n",
    "                (iris.coords.DimCoord(alt_bin_mids, standard_name=\"altitude\", units=\"m\"), 0)\n",
    "            ],\n",
    "        )\n",
    "        cb_mean.attributes[\"region\"] = region_names[ir]\n",
    "        cb_mean.attributes[\"method\"] = \"mean\"\n",
    "\n",
    "        cb_std = iris.cube.Cube(\n",
    "            vp_value[\"std\"].values.squeeze(),\n",
    "            long_name=vp_key,\n",
    "            dim_coords_and_dims=[\n",
    "                (iris.coords.DimCoord(alt_bin_mids, standard_name=\"altitude\", units=\"m\"), 0)\n",
    "            ],\n",
    "        )\n",
    "        cb_std.attributes[\"region\"] = region_names[ir]\n",
    "        cb_std.attributes[\"method\"] = \"std_dev\"\n",
    "        cb_count = iris.cube.Cube(\n",
    "            vp_value[\"count\"].values.squeeze(),\n",
    "            long_name=vp_key,\n",
    "            dim_coords_and_dims=[\n",
    "                (iris.coords.DimCoord(alt_bin_mids, standard_name=\"altitude\", units=\"m\"), 0)\n",
    "            ],\n",
    "        )\n",
    "        cb_count.attributes[\"region\"] = region_names[ir]\n",
    "        cb_count.attributes[\"method\"] = \"count\"\n",
    "\n",
    "        iris.save(\n",
    "            cb_mean, str(path_to_prcd / f\"vp_{atom_yrmn}_atom_r{ir+1}_{vp_key}_mean.nc\")\n",
    "        )  # store mean and std in separate files\n",
    "        iris.save(\n",
    "            cb_std, str(path_to_prcd / f\"vp_{atom_yrmn}_atom_r{ir+1}_{vp_key}_std_dev.nc\")\n",
    "        )  # store mean and std in separate files\n",
    "        iris.save(\n",
    "            cb_count, str(path_to_prcd / f\"vp_{atom_yrmn}_atom_r{ir+1}_{vp_key}_count.nc\")\n",
    "        )  # store mean and std in separate files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process UKCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr_to_vmr(cube_mmr, molar_mass):\n",
    "    M_air = 28.97  # molar mass of dry air [g mol-1]\n",
    "    cube_vmr = cube_mmr * (M_air / molar_mass)\n",
    "    return cube_vmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year coordinates\n",
    "iris.coord_categorisation.add_month(cb_nyrs_ch4, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_c2h6, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_c3h8, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_o3, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_oh, \"time\", name=\"month\")\n",
    "\n",
    "iris.coord_categorisation.add_month(cb_nyrs_no, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_no2, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_hono, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_ho2no2, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_hno3, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_n2o5, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_pan, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_ppan, \"time\", name=\"month\")\n",
    "\n",
    "iris.coord_categorisation.add_month(cb_nyrs_meono2, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_etono2, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_nprono2, \"time\", name=\"month\")\n",
    "iris.coord_categorisation.add_month(cb_nyrs_iprono2, \"time\", name=\"month\")\n",
    "\n",
    "# Calculate NOx and NOy\n",
    "cb_nyrs_nox = mmr_to_vmr(cb_nyrs_no, GASES[\"no\"][\"molar_mass\"]) + mmr_to_vmr(\n",
    "    cb_nyrs_no2, GASES[\"no2\"][\"molar_mass\"]\n",
    ")\n",
    "cb_nyrs_noy = (\n",
    "    mmr_to_vmr(cb_nyrs_no, GASES[\"no\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_no2, GASES[\"no2\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_hono, GASES[\"hono\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_ho2no2, GASES[\"ho2no2\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_hno3, GASES[\"hno3\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_n2o5, GASES[\"n2o5\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_pan, GASES[\"pan\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_ppan, GASES[\"ppan\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_meono2, GASES[\"meono2\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_etono2, GASES[\"etono2\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_nprono2, GASES[\"nprono2\"][\"molar_mass\"])\n",
    "    + mmr_to_vmr(cb_nyrs_iprono2, GASES[\"iprono2\"][\"molar_mass\"])\n",
    ")\n",
    "\n",
    "# Calculate monthly mean from 8 years of data\n",
    "cb_ch4 = (\n",
    "    cb_nyrs_ch4[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_c2h6 = (\n",
    "    cb_nyrs_c2h6[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_c3h8 = (\n",
    "    cb_nyrs_c3h8[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_o3 = (\n",
    "    cb_nyrs_o3[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_oh = (\n",
    "    cb_nyrs_oh[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_no = (\n",
    "    cb_nyrs_no[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_nox = (\n",
    "    cb_nyrs_nox[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_nox.rename(\"nox\")\n",
    "\n",
    "cb_noy = (\n",
    "    cb_nyrs_noy[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_noy.rename(\"noy\")\n",
    "\n",
    "cb_meono2 = (\n",
    "    cb_nyrs_meono2[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_etono2 = (\n",
    "    cb_nyrs_etono2[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_nprono2 = (\n",
    "    cb_nyrs_nprono2[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")\n",
    "cb_iprono2 = (\n",
    "    cb_nyrs_iprono2[t0:t1]\n",
    "    .extract(iris.Constraint(month=ukca_mn))\n",
    "    .aggregated_by([\"month\"], iris.analysis.MEAN)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add coordinate bounds (for calculating area weights)\n",
    "ukca2process = iris.cube.CubeList(\n",
    "    [\n",
    "        cb_ch4,\n",
    "        cb_c2h6,\n",
    "        cb_c3h8,\n",
    "        cb_o3,\n",
    "        cb_oh,\n",
    "        cb_no,\n",
    "        cb_nox,\n",
    "        cb_noy,\n",
    "        cb_meono2,\n",
    "        cb_etono2,\n",
    "        cb_nprono2,\n",
    "        cb_iprono2,\n",
    "    ]\n",
    ")\n",
    "for cube in ukca2process:\n",
    "    for coord in [\"longitude\", \"latitude\"]:\n",
    "        if not cube.coord(coord).has_bounds():\n",
    "            cube.coord(coord).guess_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area weights for each region\n",
    "any_cube = cb_meono2\n",
    "regions = []\n",
    "for corners in ukca_regions_corners:\n",
    "    llon, ulon, llat, ulat = corners\n",
    "    lonlat_constraint = iris.Constraint(\n",
    "        longitude=lambda cell: llon <= cell <= ulon, latitude=lambda cell: llat <= cell <= ulat\n",
    "    )\n",
    "    any_cube_extr = any_cube.extract(lonlat_constraint)\n",
    "    weights_cube = any_cube_extr.copy(data=iris.analysis.cartography.area_weights(any_cube_extr))\n",
    "    weights_cube.rename(\"area_weights\")\n",
    "    regions.append(\n",
    "        {\n",
    "            \"corners\": {k: v for k, v in zip([\"llon\", \"ulon\", \"llat\", \"ulat\"], corners)},\n",
    "            \"area_weights\": weights_cube,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regional mean vertical profiles and standart deviations\n",
    "stats = (\"mean\", \"std_dev\")\n",
    "ukca_regional_vps = []\n",
    "for region in regions:\n",
    "    cube_dict = {}\n",
    "    corners = region[\"corners\"]\n",
    "    lonlat_constraint = iris.Constraint(\n",
    "        longitude=lambda cell: corners[\"llon\"] <= cell <= corners[\"ulon\"],\n",
    "        latitude=lambda cell: corners[\"llat\"] <= cell <= corners[\"ulat\"],\n",
    "    )\n",
    "    for cube in ukca2process.extract(lonlat_constraint):\n",
    "        stat_dict = {}\n",
    "        for stat in stats:\n",
    "            cubelist = iris.cube.CubeList()\n",
    "            for lbound, ubound in zip(alt_bins[:-1], alt_bins[1:]):\n",
    "                alt_constraint = iris.Constraint(altitude=lambda cell: lbound < cell <= ubound)\n",
    "                cube_extr = cube.extract(alt_constraint)\n",
    "                if stat == \"mean\":\n",
    "                    kwargs = {\"weights\": region[\"area_weights\"].extract(alt_constraint).data}\n",
    "                else:\n",
    "                    kwargs = {}\n",
    "                cubelist.append(\n",
    "                    cube_extr.collapsed(\n",
    "                        [\"longitude\", \"latitude\", \"altitude\"],\n",
    "                        getattr(iris.analysis, stat.upper()),\n",
    "                        **kwargs\n",
    "                    )\n",
    "                )\n",
    "            stat_dict[stat] = cubelist.merge_cube()\n",
    "        cube_dict[cube.name()] = stat_dict\n",
    "    ukca_regional_vps.append(cube_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store UKCA vertical profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memo: ukca_regional_vps[r]['sp_key']{'mean','std_dev'}\n",
    "for ir, vp in enumerate(ukca_regional_vps):\n",
    "    for vp_key, vp_value in vp.items():\n",
    "        for var in [\"mean\", \"std_dev\"]:\n",
    "            iris.save(\n",
    "                vp_value[var],\n",
    "                str(path_to_prcd / f\"vp_{atom_yrmn}_{EXPERIMENTS[exp]}_r{ir+1}_{vp_key}_{var}.nc\"),\n",
    "            )  # store mean and std in separate files"
   ]
  }
 ],
 "metadata": {
  "@deathbeds/jupyterlab-fonts": {
   "styles": {
    ":root": {
     "--jp-code-font-size": "12px"
    }
   }
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
